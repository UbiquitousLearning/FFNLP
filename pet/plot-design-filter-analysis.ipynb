{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "import logging\n",
    "process_id = os.getpid()\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                        format=str(\n",
    "                            process_id) + ' - %(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',\n",
    "                        datefmt='%a, %d %b %Y %H:%M:%S')\n",
    "\n",
    "# nohup python sweep_aug.py --dataset agnews --device 0 --train_examples 0 --test_examples -1 --unlabeled_examples -1 --method fedpet --client_num_in_total 32 --all_client_num_in_total 1000 --seed 6 --pattern_ids 1 --alpha 1 --data_point 5 --num_clients_infer 5 --infer_freq 1 &\n",
    "\n",
    "def calculate_sentence_transformer_embedding(text_to_encode, mean=True):\n",
    "    num = len(text_to_encode)\n",
    "    emb_model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "    embeddings = []\n",
    "    # bar = tqdm(range(0,num,20),desc='calculate embeddings')\n",
    "    for i in range(0,num,20):\n",
    "        embeddings += emb_model.encode(text_to_encode[i:i+20]).tolist()\n",
    "        # bar.update(1)\n",
    "    embeddings = torch.tensor(embeddings)\n",
    "    mean_embeddings = torch.mean(embeddings, 0, True)\n",
    "    if mean:\n",
    "        embeddings = embeddings - mean_embeddings\n",
    "    else:\n",
    "        embeddings = embeddings\n",
    "    return embeddings\n",
    "\n",
    "def text_to_encode(train_examples, dataset):\n",
    "    if dataset == \"agnews\":\n",
    "        return [\"{}(){}\".format(raw_item.to_dict()[\"text_a\"], raw_item.to_dict()[\"text_b\"]) for raw_item in train_examples]\n",
    "    elif dataset == \"mnli\":\n",
    "        return [\"{}.\\nquestion: {}\".format(raw_item.to_dict()[\"text_a\"], raw_item.to_dict()[\"text_b\"]) for raw_item in train_examples]\n",
    "    elif dataset == \"yahoo\":\n",
    "        return [\"question: {}.\\nanswer: {}\".format(raw_item.to_dict()[\"text_a\"], raw_item.to_dict()[\"text_b\"]) for raw_item in train_examples]\n",
    "    elif dataset == \"yelp-full\":\n",
    "        return [\"{}\".format(raw_item.to_dict()[\"text_a\"]) for raw_item in train_examples]\n",
    "    else:\n",
    "        raise ValueError(\"dataset not supported\")\n",
    "\n",
    "def select_by_sorting(labeled_example, unlabeled_examples, select_num, dataset):\n",
    "    if len(labeled_example) == 0:\n",
    "        logging.info(\"no labeled example, select randomly\")\n",
    "        labeled_example = unlabeled_examples[0:1]\n",
    "        unlabeled_examples = unlabeled_examples[1:]\n",
    "    all_train_text_to_encode = text_to_encode(list(unlabeled_examples), dataset)\n",
    "    embeddings = calculate_sentence_transformer_embedding(text_to_encode=all_train_text_to_encode,mean=False)\n",
    "    unlabeled_embeddings = embeddings\n",
    "    all_train_text_to_encode = text_to_encode(list(labeled_example), dataset)\n",
    "    embeddings = calculate_sentence_transformer_embedding(text_to_encode=all_train_text_to_encode,mean=False)\n",
    "    labeled_embeddings = embeddings\n",
    "    # logging.info(f\"unlabeled_embeddings: {unlabeled_embeddings}, labeled_embeddings: {labeled_embeddings}\")\n",
    "    # labeled_embeddings = labeled_embeddings.reshape(1, -1)\n",
    "    # logging.info(f\"After reshaping, labeled_embeddings: {labeled_embeddings}\")\n",
    "    logging.info(\"Compute cosin_similarity.\")\n",
    "    similarity = cosine_similarity(unlabeled_embeddings, labeled_embeddings)\n",
    "    # logging.info(f\"similarity shape: {similarity.shape}, similarity: {similarity}\")\n",
    "    similarity = np.mean(similarity, axis=1)\n",
    "    # logging.info(f\"After np.mean(): similarity shape: {similarity.shape}, similarity: {similarity}\")\n",
    "    selected_indices = np.argsort(similarity)[-select_num:]\n",
    "    selected_examples = []\n",
    "    for idx in selected_indices:\n",
    "        selected_examples.append(unlabeled_examples[idx])\n",
    "    return selected_examples\n",
    "\n",
    "def select_by_voting(train_examples, select_num, output_dir, dataset, k = 150):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    vote_file=os.path.join(output_dir,'votek_cache.json')\n",
    "\n",
    "    if vote_file is not None and os.path.isfile(vote_file): # will load from json file if exists.\n",
    "        logging.info(f'load from {vote_file}')\n",
    "        embeddings=[]\n",
    "    else:\n",
    "        all_train_text_to_encode = text_to_encode(list(train_examples), dataset)\n",
    "        embeddings = calculate_sentence_transformer_embedding(text_to_encode=all_train_text_to_encode)\n",
    "\n",
    "    selected_indices = fast_votek(embeddings=embeddings,\n",
    "                                  select_num=select_num,\n",
    "                                  k=k,\n",
    "                                  vote_file=os.path.join(output_dir,'votek_cache.json'))\n",
    "    selected_examples = []\n",
    "    for idx in selected_indices:\n",
    "        selected_examples.append(train_examples[idx])\n",
    "    return selected_examples\n",
    "\n",
    "def fast_votek(embeddings,select_num,k,vote_file=None):\n",
    "    n = len(embeddings)\n",
    "    if vote_file is not None and os.path.isfile(vote_file):\n",
    "        with open(vote_file) as f:\n",
    "            vote_stat = json.load(f)\n",
    "    else:\n",
    "        # bar = tqdm(range(n),desc=f'voting')\n",
    "        vote_stat = defaultdict(list)\n",
    "        for i in range(n):\n",
    "            cur_emb = embeddings[i].reshape(1, -1)\n",
    "            # logging.info(\"Compute cosin_similarity.\")\n",
    "            cur_scores = np.sum(cosine_similarity(embeddings, cur_emb), axis=1)\n",
    "            sorted_indices = np.argsort(cur_scores).tolist()[-k-1:-1]\n",
    "            for idx in sorted_indices:\n",
    "                if idx!=i:\n",
    "                    vote_stat[idx].append(i) # idx （与i最相似的150个样本之一）的 vote_stat 里面加入 i作为相似的样本\n",
    "            # logging.info(i)\n",
    "            # bar.update(1)\n",
    "        if vote_file is not None:\n",
    "            with open(vote_file,'w') as f:\n",
    "                json.dump(vote_stat,f)\n",
    "        logging.info(f'voting done, saved into {vote_file}')\n",
    "    votes = sorted(vote_stat.items(),key=lambda x:len(x[1]),reverse=True) # 按照拥有相似样本的数量来排序\n",
    "    logging.info(f'sorted votes')\n",
    "    selected_indices = []\n",
    "    selected_times = defaultdict(int)\n",
    "    while len(selected_indices)<select_num:\n",
    "        cur_scores = defaultdict(int)\n",
    "        # logging.info(f'len(selected_indices)={len(selected_indices)}')\n",
    "        for idx,candidates in votes:\n",
    "            if idx in selected_indices:\n",
    "                cur_scores[idx] = -100\n",
    "                continue\n",
    "            for one_support in candidates:\n",
    "                if not one_support in selected_indices:\n",
    "                    cur_scores[idx] += 10 ** (-selected_times[one_support]) # if one_support not been selected, add 1, or add **.\n",
    "        cur_selected_idx = max(cur_scores.items(),key=lambda x:x[1])[0] # discourage idx that has been selected to encourage diversity.\n",
    "        selected_indices.append(int(cur_selected_idx))\n",
    "        for idx_support in vote_stat[cur_selected_idx]: # 与cur_selected_idx相关的样本的selected_times加1，support越多，权重越低，越不容易被选中\n",
    "            selected_times[idx_support] += 1\n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "class InputExample(object):\n",
    "    \"\"\"A raw input example consisting of one or two segments of text and a label\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None, logits=None, meta: Optional[Dict] = None, idx=-1):\n",
    "        \"\"\"\n",
    "        Create a new InputExample.\n",
    "\n",
    "        :param guid: a unique textual identifier\n",
    "        :param text_a: the sequence of text\n",
    "        :param text_b: an optional, second sequence of text\n",
    "        :param label: an optional label\n",
    "        :param logits: an optional list of per-class logits\n",
    "        :param meta: an optional dictionary to store arbitrary meta information\n",
    "        :param idx: an optional numeric index\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "        self.logits = logits\n",
    "        self.idx = idx\n",
    "        self.meta = meta if meta else {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serialize this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serialize this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_examples(path: str) -> List['InputExample']:\n",
    "        \"\"\"Load a set of input examples from a file\"\"\"\n",
    "        with open(path, 'rb') as fh:\n",
    "            return pickle.load(fh)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_examples(examples: List['InputExample'], path: str) -> None:\n",
    "        \"\"\"Save a set of input examples to a file\"\"\"\n",
    "        with open(path, 'wb') as fh:\n",
    "            pickle.dump(examples, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22164/3864551212.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data/cdq/pet_data/log/mnli/all_1000/seed_6/pattern_0/alpha_0_beta_0_gamma_100/new_fixed_vote_stale_aug_100_conver_0_limit_0_fedpet_64_32_roberta-base_5_1_filter_-1/g-1/client2/this-gen-train-data/train.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mipet_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pet'"
     ]
    }
   ],
   "source": [
    "import pet\n",
    "p = os.path.join(\"/data/cdq/pet_data/log/mnli/all_1000/seed_6/pattern_0/alpha_0_beta_0_gamma_100/new_fixed_vote_stale_aug_100_conver_0_limit_0_fedpet_64_32_roberta-base_5_1_filter_-1/g-1/client2/this-gen-train-data/train.bin\")\n",
    "ipet_data = InputExample.load_examples(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/cdq/Desktop/opensource/FedPrompt/data/ablation/filter/train.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_by_voting(train_examples, select_num, output_dir, dataset, k = 150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ptpretrain')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c821809210aa391b17e4d60edfc9165cf4a62d2648f08e4ac243934ff5ff366"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
