19145 - Wed, 31 Aug 2022 11:37:27 cli.py[line:233] INFO Parameters: Namespace(adam_epsilon=1e-08, aggregated=True, alpha=0.9999, augmentation=True, beta=None, cache_dir='', data_dir='/data2/cdq/pet_data/data/agnews/', decoding_strategy='default', do_eval=True, do_train=True, eval_set='dev', fed=True, ipet_generations=100, ipet_logits_percentage=0.25, ipet_n_most_likely=-1, ipet_scale_factor=2.0, learning_rate=1e-05, lm_training=False, logging_steps=5000, max_grad_norm=1.0, method='fedpet', model_name_or_path='bert-base-uncased', model_type='bert', no_cuda=False, no_distillation=False, output_dir='/data2/cdq/pet/log_fedpet_augmentation_replace_10', overwrite_output_dir=False, pattern_ids=[1], pet_gradient_accumulation_steps=1, pet_max_seq_length=256, pet_max_steps=-1, pet_num_train_epochs=10.0, pet_per_gpu_eval_batch_size=8, pet_per_gpu_train_batch_size=4, pet_per_gpu_unlabeled_batch_size=4, pet_repetitions=1, priming=False, reduction='mean', sc_gradient_accumulation_steps=1, sc_max_seq_length=256, sc_max_steps=-1, sc_num_train_epochs=3, sc_per_gpu_eval_batch_size=8, sc_per_gpu_train_batch_size=4, sc_per_gpu_unlabeled_batch_size=4, sc_repetitions=1, seed=42, split_examples_evenly=False, task_name='agnews', temperature=2, test_examples=-1, train_examples=10, unlabeled_examples=-1, vanilla=False, verbalizer_file=None, warmup_steps=0, weight_decay=0.01, wrapper_type='mlm')
19145 - Wed, 31 Aug 2022 11:37:27 tasks.py[line:827] INFO Creating features from dataset file at /data2/cdq/pet_data/data/agnews/ (num_examples=10, set_type=train)
19145 - Wed, 31 Aug 2022 11:37:28 tasks.py[line:853] INFO Returning 10 train examples with label dist.: [('1', 3), ('4', 4), ('2', 2), ('3', 1)]
19145 - Wed, 31 Aug 2022 11:37:28 tasks.py[line:827] INFO Creating features from dataset file at /data2/cdq/pet_data/data/agnews/ (num_examples=-1, set_type=dev)
19145 - Wed, 31 Aug 2022 11:37:28 tasks.py[line:853] INFO Returning 7600 dev examples with label dist.: [('3', 1900), ('4', 1900), ('2', 1900), ('1', 1900)]
19145 - Wed, 31 Aug 2022 11:37:28 tasks.py[line:827] INFO Creating features from dataset file at /data2/cdq/pet_data/data/agnews/ (num_examples=-1, set_type=unlabeled)
19145 - Wed, 31 Aug 2022 11:37:29 tasks.py[line:853] INFO Returning 120000 unlabeled examples with label dist.: [('1', 120000)]
19145 - Wed, 31 Aug 2022 11:37:29 cli.py[line:272] INFO Parameters after setting: Namespace(adam_epsilon=1e-08, aggregated=True, alpha=0.9999, augmentation=True, beta=None, cache_dir='', data_dir='/data2/cdq/pet_data/data/agnews/', decoding_strategy='default', device='cuda', do_eval=True, do_train=True, eval_set='dev', fed=True, ipet_generations=100, ipet_logits_percentage=0.25, ipet_n_most_likely=-1, ipet_scale_factor=2.0, label_list=['1', '2', '3', '4'], learning_rate=1e-05, lm_training=False, logging_steps=5000, max_grad_norm=1.0, method='fedpet', metrics=['acc'], model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, no_distillation=False, output_dir='/data2/cdq/pet/log_fedpet_augmentation_replace_10', overwrite_output_dir=False, pattern_ids=[1], pet_gradient_accumulation_steps=1, pet_max_seq_length=256, pet_max_steps=-1, pet_num_train_epochs=10.0, pet_per_gpu_eval_batch_size=8, pet_per_gpu_train_batch_size=4, pet_per_gpu_unlabeled_batch_size=4, pet_repetitions=1, priming=False, reduction='mean', sc_gradient_accumulation_steps=1, sc_max_seq_length=256, sc_max_steps=-1, sc_num_train_epochs=3, sc_per_gpu_eval_batch_size=8, sc_per_gpu_train_batch_size=4, sc_per_gpu_unlabeled_batch_size=4, sc_repetitions=1, seed=42, split_examples_evenly=False, task_name='agnews', temperature=2, test_examples=-1, train_examples=10, unlabeled_examples=-1, vanilla=False, verbalizer_file=None, warmup_steps=0, weight_decay=0.01, wrapper_type='mlm')
19145 - Wed, 31 Aug 2022 11:37:29 modeling.py[line:448] INFO All clients: sample_num_list is [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
19145 - Wed, 31 Aug 2022 11:37:29 modeling.py[line:502] INFO Gen 0: client_indexes is [2 8 4 9 1]
19145 - Wed, 31 Aug 2022 11:37:29 modeling.py[line:507] INFO Gen0: sample_num_list is [1. 1. 1. 1. 1.]
2022-08-31 11:37:44,030 - INFO - wrapper - Writing example 0
19145 - Wed, 31 Aug 2022 11:37:44 wrapper.py[line:433] INFO Writing example 0
/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/transformers/data/metrics/__init__.py:36: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
19145 - Wed, 31 Aug 2022 11:37:55 modeling.py[line:904] INFO init acc: val acc before training is 0.5921052631578947
19145 - Wed, 31 Aug 2022 11:37:55 modeling.py[line:908] INFO len of all_train_data: 1
2022-08-31 11:37:55,074 - INFO - wrapper - CDQ: n_gpu: 1, per_gpu_train_batch_size: 4, train_batch_size: 4
19145 - Wed, 31 Aug 2022 11:37:55 wrapper.py[line:234] INFO CDQ: n_gpu: 1, per_gpu_train_batch_size: 4, train_batch_size: 4
2022-08-31 11:37:55,075 - INFO - wrapper - Writing example 0
19145 - Wed, 31 Aug 2022 11:37:55 wrapper.py[line:433] INFO Writing example 0
/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
Epoch:   0%|          | 0/10 [00:00<?, ?it/s]Epoch:  10%|â–ˆ         | 1/10 [00:00<00:01,  8.83it/s]Epoch:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:00<00:00,  9.53it/s]Epoch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:00<00:00,  9.57it/s]Epoch:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:00<00:00,  9.44it/s]Epoch:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:00<00:00,  9.36it/s]Epoch:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:00<00:00,  9.36it/s]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00,  9.87it/s]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 10.07it/s]
2022-08-31 11:37:56,080 - INFO - wrapper - Writing example 0
19145 - Wed, 31 Aug 2022 11:37:56 wrapper.py[line:433] INFO Writing example 0
19145 - Wed, 31 Aug 2022 11:37:56 modeling.py[line:812] INFO Saving trained model at /data2/cdq/pet/log_fedpet_augmentation_replace_10/g0/client0...
19145 - Wed, 31 Aug 2022 11:37:56 modeling.py[line:816] INFO Saving complete
2022-08-31 11:37:56,634 - INFO - wrapper - Writing example 0
19145 - Wed, 31 Aug 2022 11:37:56 wrapper.py[line:433] INFO Writing example 0
2022-08-31 11:38:10,694 - INFO - wrapper - Writing example 10000
19145 - Wed, 31 Aug 2022 11:38:10 wrapper.py[line:433] INFO Writing example 10000
19145 - Wed, 31 Aug 2022 11:43:57 modeling.py[line:829] INFO Starting evaluation...
2022-08-31 11:43:57,200 - INFO - wrapper - Writing example 0
19145 - Wed, 31 Aug 2022 11:43:57 wrapper.py[line:433] INFO Writing example 0
19145 - Wed, 31 Aug 2022 11:44:18 modeling.py[line:851] INFO --- RESULT (pattern_id=1, iteration=0) ---
19145 - Wed, 31 Aug 2022 11:44:18 modeling.py[line:852] INFO {'acc': 0.3013157894736842}
19145 - Wed, 31 Aug 2022 11:44:18 modeling.py[line:853] INFO /data2/cdq/pet/log_fedpet_augmentation_replace_10/g0/client0
19145 - Wed, 31 Aug 2022 11:44:18 modeling.py[line:867] INFO === OVERALL RESULTS ===
19145 - Wed, 31 Aug 2022 11:44:18 modeling.py[line:992] INFO acc-p1: 0.3013157894736842 +- 0
19145 - Wed, 31 Aug 2022 11:44:18 modeling.py[line:1000] INFO acc-all-p: 0.3013157894736842 +- 0
2022-08-31 11:44:29,457 - INFO - wrapper - Writing example 0
19145 - Wed, 31 Aug 2022 11:44:29 wrapper.py[line:433] INFO Writing example 0
19145 - Wed, 31 Aug 2022 11:44:48 modeling.py[line:904] INFO init acc: val acc before training is 0.575
19145 - Wed, 31 Aug 2022 11:44:48 modeling.py[line:908] INFO len of all_train_data: 1
2022-08-31 11:44:48,455 - INFO - wrapper - CDQ: n_gpu: 1, per_gpu_train_batch_size: 4, train_batch_size: 4
19145 - Wed, 31 Aug 2022 11:44:48 wrapper.py[line:234] INFO CDQ: n_gpu: 1, per_gpu_train_batch_size: 4, train_batch_size: 4
2022-08-31 11:44:48,455 - INFO - wrapper - Writing example 0
19145 - Wed, 31 Aug 2022 11:44:48 wrapper.py[line:433] INFO Writing example 0
Epoch:   0%|          | 0/10 [00:00<?, ?it/s]Epoch:  10%|â–ˆ         | 1/10 [00:00<00:01,  5.20it/s]Epoch:  20%|â–ˆâ–ˆ        | 2/10 [00:00<00:01,  5.29it/s]Epoch:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:00<00:01,  5.71it/s]Epoch:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:00<00:00,  6.24it/s]Epoch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:00<00:00,  6.13it/s]Epoch:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:01<00:00,  5.81it/s]Epoch:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:01<00:00,  5.92it/s]Epoch:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:01<00:00,  6.10it/s]Epoch:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:01<00:00,  5.80it/s]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  6.08it/s]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  6.04it/s]
2022-08-31 11:44:50,123 - INFO - wrapper - Writing example 0
19145 - Wed, 31 Aug 2022 11:44:50 wrapper.py[line:433] INFO Writing example 0
19145 - Wed, 31 Aug 2022 11:44:50 modeling.py[line:812] INFO Saving trained model at /data2/cdq/pet/log_fedpet_augmentation_replace_10/g0/client1...
19145 - Wed, 31 Aug 2022 11:44:50 modeling.py[line:816] INFO Saving complete
2022-08-31 11:44:50,678 - INFO - wrapper - Writing example 0
19145 - Wed, 31 Aug 2022 11:44:50 wrapper.py[line:433] INFO Writing example 0
2022-08-31 11:45:04,603 - INFO - wrapper - Writing example 10000
19145 - Wed, 31 Aug 2022 11:45:04 wrapper.py[line:433] INFO Writing example 10000
19145 - Wed, 31 Aug 2022 11:51:12 modeling.py[line:829] INFO Starting evaluation...
2022-08-31 11:51:12,809 - INFO - wrapper - Writing example 0
19145 - Wed, 31 Aug 2022 11:51:12 wrapper.py[line:433] INFO Writing example 0
19145 - Wed, 31 Aug 2022 11:51:39 modeling.py[line:851] INFO --- RESULT (pattern_id=1, iteration=0) ---
19145 - Wed, 31 Aug 2022 11:51:39 modeling.py[line:852] INFO {'acc': 0.2578947368421053}
19145 - Wed, 31 Aug 2022 11:51:39 modeling.py[line:853] INFO /data2/cdq/pet/log_fedpet_augmentation_replace_10/g0/client1
19145 - Wed, 31 Aug 2022 11:51:39 modeling.py[line:867] INFO === OVERALL RESULTS ===
19145 - Wed, 31 Aug 2022 11:51:39 modeling.py[line:992] INFO acc-p1: 0.2578947368421053 +- 0
19145 - Wed, 31 Aug 2022 11:51:39 modeling.py[line:1000] INFO acc-all-p: 0.2578947368421053 +- 0
2022-08-31 11:51:49,047 - INFO - wrapper - Writing example 0
19145 - Wed, 31 Aug 2022 11:51:49 wrapper.py[line:433] INFO Writing example 0
19145 - Wed, 31 Aug 2022 11:52:08 modeling.py[line:904] INFO init acc: val acc before training is 0.5947368421052631
19145 - Wed, 31 Aug 2022 11:52:08 modeling.py[line:908] INFO len of all_train_data: 1
2022-08-31 11:52:08,247 - INFO - wrapper - CDQ: n_gpu: 1, per_gpu_train_batch_size: 4, train_batch_size: 4
19145 - Wed, 31 Aug 2022 11:52:08 wrapper.py[line:234] INFO CDQ: n_gpu: 1, per_gpu_train_batch_size: 4, train_batch_size: 4
2022-08-31 11:52:08,247 - INFO - wrapper - Writing example 0
19145 - Wed, 31 Aug 2022 11:52:08 wrapper.py[line:433] INFO Writing example 0
Epoch:   0%|          | 0/10 [00:00<?, ?it/s]Epoch:  10%|â–ˆ         | 1/10 [00:00<00:01,  5.80it/s]Epoch:  20%|â–ˆâ–ˆ        | 2/10 [00:00<00:01,  5.84it/s]Epoch:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:00<00:01,  5.80it/s]Epoch:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:00<00:01,  5.72it/s]Epoch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:00<00:00,  5.94it/s]Epoch:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:00<00:00,  6.19it/s]Epoch:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:01<00:00,  6.28it/s]Epoch:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:01<00:00,  5.78it/s]Epoch:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:01<00:00,  5.65it/s]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.53it/s]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.78it/s]
2022-08-31 11:52:09,989 - INFO - wrapper - Writing example 0
19145 - Wed, 31 Aug 2022 11:52:09 wrapper.py[line:433] INFO Writing example 0
19145 - Wed, 31 Aug 2022 11:52:10 modeling.py[line:812] INFO Saving trained model at /data2/cdq/pet/log_fedpet_augmentation_replace_10/g0/client2...
19145 - Wed, 31 Aug 2022 11:52:10 modeling.py[line:816] INFO Saving complete
2022-08-31 11:52:10,544 - INFO - wrapper - Writing example 0
19145 - Wed, 31 Aug 2022 11:52:10 wrapper.py[line:433] INFO Writing example 0
2022-08-31 11:52:24,621 - INFO - wrapper - Writing example 10000
19145 - Wed, 31 Aug 2022 11:52:24 wrapper.py[line:433] INFO Writing example 10000
Traceback (most recent call last):
  File "cli.py", line 311, in <module>
    main()
  File "cli.py", line 299, in main
    eval_data=eval_data, do_train=args.do_train, do_eval=args.do_eval, seed=args.seed, aggregated=args.aggregated, vanilla=args.vanilla, fed=args.fed, augmentation=args.augmentation, beta=args.beta)
  File "/home/cdq/FedPet/pet/modeling.py", line 615, in train_fedpet
    eval_data=eval_data, do_train=do_train, do_eval=do_eval, save_unlabeled_logits=augmentation, aggregated_model_path = aggregated_model_path) 
  File "/home/cdq/FedPet/pet/modeling.py", line 819, in train_pet_ensemble
    logits = evaluate(wrapper, unlabeled_data, eval_config)['logits']
  File "/home/cdq/FedPet/pet/modeling.py", line 963, in evaluate
    n_gpu=config.n_gpu, decoding_strategy=config.decoding_strategy, priming=config.priming)
  File "/home/cdq/FedPet/pet/wrapper.py", line 386, in eval
    # some tasks require special evaluation
  File "/home/cdq/FedPet/pet/wrapper.py", line 534, in mlm_eval_step
    return outputs[0]
  File "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py", line 1354, in forward
    return_dict=return_dict,
  File "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py", line 1006, in forward
    return_dict=return_dict,
  File "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py", line 592, in forward
    output_attentions,
  File "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py", line 514, in forward
    self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output
  File "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/site-packages/transformers/modeling_utils.py", line 2897, in apply_chunking_to_forward
    num_args_in_forward_chunk_fn = len(inspect.signature(forward_fn).parameters)
  File "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/inspect.py", line 3070, in signature
    return Signature.from_callable(obj, follow_wrapped=follow_wrapped)
  File "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/inspect.py", line 2820, in from_callable
    follow_wrapper_chains=follow_wrapped)
  File "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/inspect.py", line 2210, in _signature_from_callable
    sigcls=sigcls)
  File "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/inspect.py", line 2277, in _signature_from_callable
    return _signature_from_function(sigcls, obj)
  File "/home/cdq/.conda/envs/ptpretrain/lib/python3.7/inspect.py", line 2157, in _signature_from_function
    if func_code.co_flags & CO_VARARGS:
KeyboardInterrupt
